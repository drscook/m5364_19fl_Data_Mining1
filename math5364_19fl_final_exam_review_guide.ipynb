{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "math5364_19fl_final_exam_review_guide.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drscook/m5364_19fl_Data_Mining1/blob/master/math5364_19fl_final_exam_review_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXyBjE1VNF3S",
        "colab_type": "text"
      },
      "source": [
        "# Final Exam Review Guide\n",
        "## Math 5364 - Data Mining 1\n",
        "## Fall 2019 - Dr. Scott Cook - Tarleton State Univ\n",
        "## Version 2019/11/29\n",
        "\n",
        "### Linear Algebra Definitions - give precise definitions\n",
        "- linear independence\n",
        "- span\n",
        "- basis\n",
        "- dimension\n",
        "- linear transformation\n",
        "- orthogonal/orthonormal vectors\n",
        "- eigenvectors/values\n",
        "- adjoint of an operator\n",
        "- normal operator\n",
        "- self-adjoint operator\n",
        "- Spectral Theorem\n",
        "- Principle Components\n",
        "\n",
        "\n",
        "### Data Science definitions\n",
        "- parameter vs hyperparameter\n",
        "- features/input/indpendent/predictor vs target/output/dependent/response\n",
        "- categorical vs continuous variables\n",
        "- classifier vs regressor\n",
        "- eager vs lazy\n",
        "- supervised vs unsupervised\n",
        "- model ealuation\n",
        "  - classification accuracy\n",
        "  - confusion matrix\n",
        "  - sensitivity/recall, specificity, precision, $F_1$, $F_\\beta$\n",
        "  - Receiver-Operator Curve (ROC) curve & Area Under the Curve (AUR)\n",
        "  - cost matrix\n",
        "\n",
        "\n",
        "### Big Ideas - Describe and give examples\n",
        "- Data Science \"big picture\" flow chart\n",
        "- underfitting (bias) vs overfitting (variance)\n",
        "- decision boundary\n",
        "- cross-validation\n",
        "  - Why we do it\n",
        "  - Compare/contrast 4 different types of CV\n",
        "- feature engineering\n",
        "  - one-hot-encoding (dummy variables)\n",
        "  - imputation schemes (and how to choose)\n",
        "- Model Tuning/optimization\n",
        "\n",
        "\n",
        "### Specific Algorithms\n",
        "For each:\n",
        "  - give a high-level description and picture summarizing how it works\n",
        "  - describe when it can be used.  For example, what type(s) of features can it handle, do all features need to be scaled to mean 0 variance 1, must features must be approximately normal, etc?\n",
        "  - identify key hyperparameters, describe the meaning/significance/role in the algorithm, effect when you change it\n",
        "  - answer specific questions below\n",
        "\n",
        "1. Principal Components\n",
        "  - What is $U$?  How do you get it and how do you use it?\n",
        "  - How do you compute the variance explained by the first $k$ principal components?\n",
        "\n",
        "1. k-Nearest Neighbors\n",
        "  - What happens as $k$ varies?\n",
        "\n",
        "1. k-means\n",
        "  - Describe/illustrate the expectation-maximization process used to find clusters\n",
        "\n",
        "1. Gaussian Mixtures\n",
        "  - How does it generalize k-means\n",
        "\n",
        "1. Naive Bayes Classifier\n",
        "  - Name at least 2 types of NB and the situation where they apply\n",
        "  - We went deep into the situation with all categorical variables.  For this situation:\n",
        "    - State Bayes Theorem\n",
        "    - Give the \"not-naive\" Bayes Classifier equation and explain why we can't use it.\n",
        "    - State the \"naive\" assumption and use it to turn the \"not-naive\" Bayes expression above into the naive Bayes expression we actually use.\n",
        "\n",
        "1. Decision Trees\n",
        "  - State 3 different common impurity measures.\n",
        "  - Discuss choices that must be made, such as\n",
        "    - group levels of a categorical feature\n",
        "    - where to partition continuous features\n",
        "    - stopping criteria\n",
        "    - Recall that each implementation (CART, ID3, C4.5, etc) has its own set of rules for making these choices.  You are NOT responsible to describe those rules; just describe the issues.\n",
        "  - Why is it bad to use 1 decision tree by itself?  What is the improved version called and why is it better (in general terms)?\n",
        "\n",
        "1. Support Vector Machines\n",
        "  - Totally Linearly Separable case\n",
        "    - What is $\\vec{\\beta}$?\n",
        "    - What is $\\vec{x}_0$?  (Note: I switched from $\\vec{x}_0$ on the board to $\\vec{z}$ in the notes I handed out for the same thing.)\n",
        "    - State the \"original\" optimization problems (OPT1 in notes) and explain what each part means.\n",
        "    - Names associated to the \"conditions\" that let us convert OPT3 into OPT4\n",
        "    - Equation e was critical because it split rows into 2 cases.  Describe these 2 cases and why the first case dramatically simplifies the problem.\n",
        "  - ~~Totally~~ Linearly Separable case\n",
        "    - State the new optimization problem and describe the new variables (slack and cost)\n",
        "  - ~~Totally~~ ~~Linearly~~ Separable case\n",
        "    - What is $\\vec{h}(\\vec{x})$?\n",
        "    - State the big idea (3 steps)\n",
        "    - \"This big idea is typically too computationally expensive.  But, for some special choices of $\\vec{h}(\\vec{x})$, there is a computation efficient efficent called the _ _ _ _ _ _ _ _ _ _ _ _ _ _ _.\""
      ]
    }
  ]
}