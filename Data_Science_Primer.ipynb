{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Science Primer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drscook/m5364_19fl_Data_Mining1/blob/master/Data_Science_Primer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRwgqup1L62b",
        "colab_type": "text"
      },
      "source": [
        "# Almost None of What You Should Know About Data Science\n",
        "August 27, 2019\n",
        "Tarleton State University\n",
        "Math 5364 - Data Mining I\n",
        "\n",
        "This primer was put together in great haste.\n",
        "- Every list is incomplete\n",
        "- *Many* important topics are not even hinted at\n",
        "- Every topic that is discussed is oversimplified\n",
        " \n",
        "But I hope it gets you started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CorY_CvfDx24",
        "colab_type": "text"
      },
      "source": [
        "# What is Data Science?\n",
        "\n",
        "Finding patterns in piles of data.\n",
        "\n",
        "# Why is Data Science important?\n",
        "I believe data science represents the next great evolution in how humans figure stuff out.  But it's a lot messier than its predecessors.\n",
        "\n",
        "- Greeks - logic & reason & \"Platonic ideals\"\n",
        "- Medieval - sacred texts\n",
        "- Renaissance - scientific experiments\n",
        "    - Enormous improvement, but there are a lot of things we can't design and run experiments on.\n",
        "- 1800 \\& 1900's - Theoretical frameworks\n",
        "    - Ex: Einstein's general relativity - built on a beautiful conceptual foundation from the brain of a genius.  Amazing, but not available to most people/organizations.\n",
        "- Now - data science\n",
        "    - Trying to find patterns answers in real world \"observational\" data that doesn't come from a controlled experiment without the benefit of an ingenious theoretical framework.  The data is not clean, it has missing data, it has errors, it has confounding variables, etc.  And we don't have Einstein ... but we do have powerful computers.\n",
        "    \n",
        "# How is it different from statistics?\n",
        "\n",
        "It's blurry.\n",
        "\n",
        "Machine Learning grew out of computer science and statistics grew from math.  They overlap a lot.  Data Science includes them both and more.  Perhaps it is easiest to describe what one part lacks in the absence of the other.\n",
        "\n",
        "A data scientist without a stats background is prone to playing fast and loose with model assumptions and mathematical rigor.  This can lead to overgeneralization and invalid use of a model when assumptions are not valid. (ex: the 2008 financial crisis fueled by disregard for model assumptions in housing derivative models).\n",
        "\n",
        "A statistician without a data science background tends to struggle with practical and implementation obstacles such as data cleaning/pre-processing, writing good code, taking full advantage of computational resources, finding \"good enough\" solutions, etc.\n",
        "\n",
        "But most stats/data science students learn skills in both.  These are simply poles of the continuum.\n",
        "\n",
        "# Why is Data Science hard?\n",
        "\n",
        "- Breadth of Knowledge - You need pretty substantial knowledge in\n",
        "    - Math/Stats\n",
        "    - Coding\n",
        "    - Data cleaning, processing, and warehousing\n",
        "    - The subject area where the data and questions come from\n",
        "\n",
        "- Hardware/platform changes\n",
        "    - Local vs cloud\n",
        "    - WTF is a TPU?  I just got good with GPUs?\n",
        "\n",
        "- Volume of \"major\" techniques (below)\n",
        "\n",
        "- Volume of domain specific tools \n",
        "    - Natural language processing\n",
        "    - Image processing\n",
        "    - Web scraping\n",
        "    - Data cleaning/validation\n",
        "    - Hadoop\n",
        "    - ...\n",
        "\n",
        "- Consumers - People see \"big data\" as a magic wand to wave toward whatever problem they have.\n",
        "    - Expectations too high\n",
        "    - Insufficient/unusable data\n",
        "    - Miscommunication (consumer said \"xxx\", but really wanted \"yyy\", and data scientist heard \"zzz\" where $xxx \\neq yyy \\neq zzz$)\n",
        "\n",
        "I teach Data Science.  I love teaching it.  I also hate teaching it because I have to make huge changes every time I teach it to even *attempt* to stay current.  (I usually fail at this).\n",
        "\n",
        "# Course Philosophy\n",
        "The field is enormous.  There are hundreds of niche's to address the specific needs of a vast and diverse set of users/organizations.  We can not possibly cover everything.  (And even if we did, everything would be out of date before the class ends in May anyway).\n",
        "\n",
        "Goals\n",
        "1. Set a solid foundation of core data science concepts\n",
        "1. Survey as many specific niches as possible\n",
        "\n",
        "While I will largely control how we achieve goal 1, you will help drive goal 2.\n",
        "\n",
        "I view this class more like a research group than a traditional course.  You all have strengths/knowlege/interests that no one else in the class has (including me).  \n",
        "\n",
        "# Terms\n",
        "- Variable Roles\n",
        "    - *Feature/Input/Predictor/Independent* variable - goes into the model\n",
        "    - *Target/Output/Dependent/Response* variable - thing the model is trying to predict\n",
        "- Variable Types\n",
        "    - *Categorical/Qualitative* - Head/Tails, on-time/delayed, claim/no claim, etc\n",
        "        - If target variable is categorical, model called a *classifier*\n",
        "    - Continuous/Quantitative - yield, weight, temperature, etc\n",
        "        - If target variable is continuous, model called a *regressor*\n",
        "- Supervision\n",
        "    - A *supervised* model knows the \"true\" output values and tries to to mimick them as closely as possible\n",
        "    - An *unsupervised* model does NOT know \"true\" output values.  (Ex: clustering tries to \"detect\" clusters that we didn't previously know were there.)\n",
        "- Parameter vs Hyperparameter\n",
        "    - Many learning algorthms involve *parameters* that must be fit to training data.  The computer does this.\n",
        "    - Many learning algorthms involve *hyperparameters* that must be selected by the data scientist (model selection).  The computer might help, say by trying many different options and reporting the results.  But, ultimately, a human makes the choice.\n",
        "    - Ex: Two continuous variables x = feature, y = target.  Make scatter plot.  Try to fit a curve. Options:\n",
        "        - Fit a line: $y= \\beta_0 + \\beta_1 x$\n",
        "        - Fit a quadratic: $y= \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n",
        "        - Fit a cubic: $y= \\beta_0 + \\beta_1 x + \\beta_2 x^2+ \\beta_3 x^3$\n",
        "        - ...\n",
        "        - Degree (highest power of $x$) is a hyperparameter (human picks); the $\\beta's$ are parameters (fit to training data by computer)\n",
        "        - Human can ask the computer to try each of the options and then look at the resulting model performance.  But the human must decide which one to use.  See under vs over fitting.\n",
        "- Eager vs Lazy\n",
        "    - An *eager* learner does the hard work before it get applied to new data.  It might take a long time to train an eager learner, but, after that, it usually works quickly on new data.\n",
        "    - A *lazy* learner does the hard work when it gets applied to new data.\n",
        "\n",
        "\n",
        "- Major Learning Algorithms (incomplete list)\n",
        "    - Supervised\n",
        "        - Lazy\n",
        "            - k-Nearest Neighbors\n",
        "            - Naive Bayes\n",
        "        - Eager\n",
        "            - Linear & Logistic Regression\n",
        "            - Decision Trees/Random Forests\n",
        "            - Support Vector Machines\n",
        "            - Ensemble Methods\n",
        "            - Reinforcement Learning\n",
        "            - Artificial Neural Networks/Deep Learning (https://www.analyticsindiamag.com/6-types-of-artificial-neural-networks-currently-being-used-in-todays-technology/)\n",
        "                - Feedforward\n",
        "                - Convolutional\n",
        "                - Recurrent\n",
        "                - Radial Basis Functions\n",
        "                - Kohonen Self Organizing\n",
        "                - Modular\n",
        "        - ...\n",
        "    - Unsupervised\n",
        "        - Principle Components Analysis\n",
        "        - k-means clustering\n",
        "        - Gaussian Mixture\n",
        "        - Manifold learning\n",
        "        - ...\n",
        "\n",
        "\n",
        "# \"Big Concepts\" in Data Science \n",
        "\n",
        "- Cross-Validation\n",
        "- Under-fitting vs Over-fitting (bias vs variance)\n",
        "- Feature Engineering/Selection & the curse of dimensionality\n",
        "- Parameters vs Hyperparameter Optimization\n",
        "- Eager vs Lazy Learners\n",
        "- Classification vs Regression\n",
        "- Weak vs Strong learners (ensemble methods)\n",
        "- Fundamental idea vs improvements vs implementation optimizations\n",
        "\n",
        "# Major Software\n",
        "\n",
        "- Python\n",
        "    - NumPy\n",
        "    - SciPy\n",
        "    - Sci-kit Learn\n",
        "    - TensorFlow & Keras\n",
        "    - NLTK toolkit\n",
        "    - Visualization\n",
        "        - Matplotlib\n",
        "        - Bokeh\n",
        "        - Seaborn\n",
        "- R\n",
        "- Jupyter\n",
        "- Hadoop\n",
        "- Sql\n",
        "- So much more ...\n",
        "\n",
        "# References\n",
        "\n",
        "- https://github.com/jakevdp/PythonDataScienceHandbook\n",
        "- https://web.stanford.edu/~hastie/Papers/ESLII.pdf\n",
        "- https://github.com/rasbt/python-machine-learning-book-2nd-edition\n",
        "- https://github.com/jakevdp/WhirlwindTourOfPython\n"
      ]
    }
  ]
}